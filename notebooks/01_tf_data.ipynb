{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac31f2df",
   "metadata": {},
   "source": [
    "# Data processing and sampling with tensorflow data input pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6912b17",
   "metadata": {},
   "source": [
    "The aim of this tutorial is to learn how to:\n",
    "- read 3D images in nifiti format for training a neural network\n",
    "- use tensorflow Dataset for efficient sampling of mini batches and on the fly data augmentation\n",
    "\n",
    "This tutorial uses simulated PET/MR data and a network that has two input channels and one output channel. However, the basic concept of using a tensor data input pipeline generalizes easily to other examples using different data, different dimensions, or a different number of channels.\n",
    "\n",
    "**Before you run this notebook: Make sure that have you run the** ```00_introduction.ipynb``` **notebook which downloads the data needed in the following notebookes into the folder** ```brainweb_petmr```\n",
    "\n",
    "This tutorial is inspired by  this keras tutorial https://keras.io/examples/vision/3D_image_classification/ on 3D CT image classifiction, which is also highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a173be0",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Efficient data sampling and data augmentation are crucial when training convolutional neural networks (CNN) - especially when using 3D images. Tensorflow offers the tf.data.Dataset class which allows to do that in very elegant and efficient way. In the following we will read a few simulated PET and MR data sets and demonstrate how to setup a tensorflow Dataset pipeline with on-the-fly data augmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f5e49",
   "metadata": {},
   "source": [
    "## Data set\n",
    "\n",
    "The data set we will use consists of 20 subjects derived from the brainweb phantom. \n",
    "\n",
    "For each subject the data is organized as follows:\n",
    "- subjectXX\n",
    "  - mu.nii.gz -> (attenuation image)\n",
    "  - t1.nii.gz -> (high resolution and low noise T1 MR)\n",
    "  - sim_0 -> first simulated PET acquisition\n",
    "    - true_pet.nii.gz -> true tracer uptated\n",
    "    - osem_psf_counts_0.0E+00.nii.gz -> OSEM recon of simulated noise free data\n",
    "    - osem_psf_counts_1.0E+07.nii.gz -> OSEM recon of simulated noisy data (1e7 counts) -> high noise level\n",
    "    - osem_psf_counts_5.0E+08.nii.gz -> OSEM recon of simulated noisy data (5e8 counts) -> low noise level\n",
    "\n",
    "  - sim_1 -> second simulated PET acquisition\n",
    "    - osem_psf_counts_0.0E+00.nii.gz\n",
    "    - osem_psf_counts_1.0E+07.nii.gz\n",
    "    - osem_psf_counts_5.0E+08.nii.gz\n",
    "    - true_pet.nii.gz\n",
    "  - sim_2 -> third simulated PET acquisition\n",
    "    - osem_psf_counts_0.0E+00.nii.gz\n",
    "    - osem_psf_counts_1.0E+07.nii.gz\n",
    "    - osem_psf_counts_5.0E+08.nii.gz\n",
    "    - true_pet.nii.gz\n",
    "\n",
    "All data sets have a shape of (256,256,258) and a voxel size of 1mm x 1mm x 1mm and are provided in nifti format. All PET acquisitions have different contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cce76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules that we need for this tutorial\n",
    "\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "# enable interactive plots with the ipympl package\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda01c8a",
   "metadata": {},
   "source": [
    "Make sure that the simulated brainweb PET/MR data sets were downloaded and that the main data path in the cell below is correct. Let's first find all data directories. In this tutorial **we load only the first 4 subjects to speed up execution**. Since there are 3 simulated acquisitions per subject, we will get in total 4*3 = 12 data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this variable to the path where the simulated PET/MR data from zenodo was unzipped\n",
    "data_dir   = pathlib.Path('brainweb_petmr')\n",
    "batch_size = 10\n",
    "nsubjects  = 4\n",
    "\n",
    "# get the paths of the first nsubjects subjects\n",
    "# we only use a few subjects in this tutorial to speed up the data reading\n",
    "subject_paths = sorted(list(data_dir.glob('subject??')))[:nsubjects]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd4a11",
   "metadata": {},
   "source": [
    "Each simulated data set contains a low resolution and noisy standard OSEM PET reconstruction, a high resolution and low noist T1 MR, and a high resolution and low noise target reconstruction. All images volumes are saved in nifti format. Let's define a first helper function that uses nibabel to load a 3D nifti volume in defined orientation (LPS). The standard orientation of nifti is RAW which is why we have to flip the 0 and 1 axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii_in_lps(fname):\n",
    "  \"\"\" function that loads nifti file and returns the volume and affine in \n",
    "      LPS orientation\n",
    "  \"\"\"\n",
    "  nii = nib.load(fname)\n",
    "  nii = nib.as_closest_canonical(nii)\n",
    "  vol = np.flip(nii.get_fdata(), (0,1))\n",
    "\n",
    "  return vol, nii.affine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a91e8a",
   "metadata": {},
   "source": [
    "When training neural networks, it is important to normalize the intensity of the input data. In the tutorial, we use a robust maximum which is the maximum of a heavily smoothed version of the input volume. The smoothing is important when working with noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f407e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_max(volume, n = 7):\n",
    "    \"\"\" function that return the max of a heavily smoothed version of the input volume\n",
    "        \n",
    "        for the smoothing we use tensorflows strided average pooling (which is faster compared to the numpy / scipy implementation) \n",
    "    \"\"\"\n",
    "    # to use tf's average pooling we first have to convert the numpy array to a tf tensor\n",
    "    # for the pooling layers, the shape of the input need to be [1,n0,n1,n2,1]\n",
    "    t = tf.convert_to_tensor(np.expand_dims(np.expand_dims(volume,0),-1).astype(np.float32))\n",
    "    \n",
    "    return tf.nn.avg_pool(t,2*n + 1,n,'SAME').numpy().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32157e",
   "metadata": {},
   "source": [
    "Let's define another helper function that loads all 3 nifiti volumes of a data set and that also already performs an intensity normalization. For the latter, we divide both PET images by the \"robust\" max of the input PET image, and the MR by its \"robust\" max, where \"robust\" max is the maximum of a heavily downsamped (pooled) volume. This is more stable when working with very noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810933b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(subject_path, sim = 0, counts = 1e7):\n",
    "\n",
    "  # get the subject number from the path\n",
    "  data_id = int(subject_path.parts[-1][-2:])\n",
    "\n",
    "  # setup the file names\n",
    "  mr_file   = pathlib.Path(subject_path) / 't1.nii.gz'\n",
    "  osem_file = pathlib.Path(subject_path) / f'sim_{sim}' / f'osem_psf_counts_{counts:0.1E}.nii.gz'\n",
    "  target_file = pathlib.Path(subject_path) / f'sim_{sim}' / 'true_pet.nii.gz'\n",
    "\n",
    "  # load nifti files in RAS orientation\n",
    "  mr, mr_aff = load_nii_in_lps(mr_file)\n",
    "  osem, osem_aff = load_nii_in_lps(osem_file)\n",
    "  target, target_aff = load_nii_in_lps(target_file)\n",
    "\n",
    "  # normalize the intensities of the MR and PET volumes\n",
    "  mr_scale   = robust_max(mr)\n",
    "  osem_scale = robust_max(osem)\n",
    "\n",
    "  mr     /= mr_scale\n",
    "  osem   /= osem_scale\n",
    "  target /= osem_scale\n",
    "\n",
    "  return osem, mr, target, osem_scale, mr_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426826be",
   "metadata": {},
   "source": [
    "In many CNN training scenarios, on-the-fly data augmentation (e.g. cropping, rotating, change of contrast) is very useful. Moreover, when working with 3D data sets, networks are often trained on smaller patches due to **memory limitations on the available GPUs**. \n",
    "Here, we define a function that samples a random 3D patch from the entire input and target data sets. For the sampling we make use of ```tf.image.random_crop``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augmentation(x, y, s0 = 64, s1 = 64, s2 = 64):\n",
    "  \"\"\"data augmentation function for training \n",
    "     \n",
    "     the input x has shape (n0,n1,n2,2) and the input y has shape (n0,n1,n2,1)\n",
    "  \"\"\"\n",
    "\n",
    "  # do the same random crop of input and output\n",
    "  z = tf.concat([x,y], axis = -1)\n",
    "  z_crop = tf.image.random_crop(z, [s0,s1,s2,z.shape[-1]])\n",
    "\n",
    "  x_crop = z_crop[...,:2]\n",
    "  y_crop = z_crop[...,2]\n",
    "\n",
    "  return x_crop, y_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ac682",
   "metadata": {},
   "source": [
    "Now let's loop over all data directories and let's store all images in 2 big numpy arrays. The first array ```x_train``` should contain the input and the second array ```y_train``` the target for our CNN during training. When working with 3D volumes, the shape of the input and output to the CNN has to be ```(nbatch,n0,n1,n2,nchannels)``` where ```nbatch``` is the mini batch length, ```n0,n1,n2``` are the spatial dimentions, and ```nchannels``` are the number of input channels. In this example, we have two input channels (OSEM PET and T1 MR) and one output channel (target PET image).\n",
    "\n",
    "Reading one data set takes up to 5s due to calculation of the robust maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7da3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the input data sets\n",
    "# we apply a slight crop to exclude background regions \n",
    "\n",
    "x = np.zeros((3*len(subject_paths),176,196,178,2), dtype = np.float32)\n",
    "y = np.zeros((3*len(subject_paths),176,196,178,1), dtype = np.float32)\n",
    "\n",
    "# load all the data sets and sort them into the x and y numpy arrays\n",
    "for i,subject_path in enumerate(subject_paths):\n",
    "  for sim in range(3):\n",
    "    print(f'loading {subject_path} simulation {sim}')\n",
    "    data = load_data_set(subject_path, sim = sim, counts = 1e7)\n",
    "    \n",
    "    # - for every subject we have 3 simulated acquistions such that the position of the current acq. is 3*i + sim\n",
    "    # - [40:-40,30:-30,40:-40] is used to crop the image in every direction to ignore empty background regions\n",
    "    #   which saves memory and avoid sampling of too many (small) empty patches\n",
    "    x[3*i + sim,...,0] = data[0][40:-40,30:-30,40:-40]\n",
    "    x[3*i + sim,...,1] = data[1][40:-40,30:-30,40:-40]\n",
    "    y[3*i + sim,...,0] = data[2][40:-40,30:-30,40:-40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3bcea",
   "metadata": {},
   "source": [
    "After we have read some of the available data sets, let's visualize the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymirc.viewer as pv\n",
    "vi = pv.ThreeAxisViewer([x[...,0].squeeze(), x[...,1].squeeze(), y[...,0].squeeze()],\n",
    "                           imshow_kwargs = {'vmin':0,'vmax':1.4}, rowlabels = [f'input 0', f'input 1', f'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1f2b2",
   "metadata": {},
   "source": [
    "Let's create a tensorflow data set from our numpy arrays stored in the host memory ```x_train, y_train```. Moreover, we use the ```shuffle``` and ```map``` methods to shuffle the data and to apply our defined data augmentation function on the fly. More information on the tensorflow dataset class can be found here: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49350c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_dataset = train_loader.shuffle(len(x)).map(lambda x,y: train_augmentation(x,y, s0 = 64, s1 = 64, s2 = 64)).batch(batch_size).prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ebf1b",
   "metadata": {},
   "source": [
    "Finally, let's draw a mini-batch and let's visualize all 3D data sets in the mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = list(train_dataset.take(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade0512",
   "metadata": {},
   "source": [
    "You can click in the plots and use your arrow keys to move through the slices / samples in the mini batch. The left/right arrow keys move through the samples, and the top/down arrow keys move throught the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = pv.ThreeAxisViewer([x_batch[...,0].numpy().squeeze(), x_batch[...,1].numpy().squeeze(), \n",
    "                         y_batch.numpy().squeeze()],\n",
    "                         imshow_kwargs = {'vmin':0,'vmax':1.4}, rowlabels = [f'input 0', f'input 1', f'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf68b07",
   "metadata": {},
   "source": [
    "## Now it's your turn - recommended exercise\n",
    "Now it is your turn, to familiarize yourself with the tensorflow dataset input pipeline:\n",
    "1. Create a 2nd training data set loader similar to ```train_dataset``` that samples random patches with different size (e.g. 128,128,128)\n",
    "2. Write your own on-the-fly data augmentation function that randomly changes the contrast of the input MR image (2nd channel). To do so, have a look into https://www.tensorflow.org/api_docs/python/tf/image/random_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e2cde",
   "metadata": {},
   "source": [
    "## What's next\n",
    "In the following notebooks we will learn:\n",
    "- how to setup a simple convolutional neural network (CNN) in tensorflow\n",
    "- how to train a CNN with our data input pipeline\n",
    "- how to monitor training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
