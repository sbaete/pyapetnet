{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pyapetnet to predict anatomy-guided MAP PET reconstructions in image space\n",
    "\n",
    "In this notebook, we wil learn how to use pre-trained models included in the pyapetnet package to predict anatomy-guided MAP PET reconstructions from (simulated) PET OSEM and T1 MR images.\n",
    "\n",
    "In this tutorial, we will have a closer look at:\n",
    "- loading pre-trained models\n",
    "- loading nifti data\n",
    "- pre-processing nifti data\n",
    "- feeding the pre-processed data into the pre-trained model\n",
    "- saving visualizing the results\n",
    "\n",
    "**If you install pyapetnet from pypi using ```pip install pyapetnet```**, it will create a command line tool that does all those steps in one go. Moreover, it allows allows to load and write dicom data.\n",
    "\n",
    "For more details on pyapetnet is available here:\n",
    "- https://doi.org/10.1016/j.neuroimage.2020.117399 (NeuroImage publication on pyapetnet)\n",
    "- https://github.com/gschramm/pyapetnet/ (github repository of pyapetnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwNWY7nD3x4T"
   },
   "source": [
    "## (1) Preparation: Install the pyapetnet package\n",
    "\n",
    "Before running this notebook, make sure that the pyapetnet package is installed.\n",
    "This can by done via <br>\n",
    "```pip install pyapetnet``` <br> \n",
    "which will install the package and all its dependencies (e.g. tensorflow). We recommend to use a separate virtual environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNVA9mz9cCet"
   },
   "source": [
    "## (2) Data used in this demo\n",
    "\n",
    "In this tutorial, we wil use simulated PET and MR data that are based on the brainweb phantom.\n",
    "The nifti files used in this tutorial, are available at <br>\n",
    "https://github.com/gschramm/pyapetnet/tree/master/demo_data <br>\n",
    "By changing ```pet_fname``` or ```mr_fname``` other input data sets can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Loading modules\n",
    "In the next cell, we will load all required python modules. E.g. tensorflow, to load the pre-trained model and pyapetnet for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyapetnet\n",
    "from pyapetnet.preprocessing import preprocess_volumes\n",
    "from pyapetnet.utils         import load_nii_in_ras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Specification of input pameters\n",
    "In the next cell, we specify the required input parameters:\n",
    "- ```model_name``` (name of the pre-trained model shipped with the pyapernet package)\n",
    "- ```pet_fname / mr_fname``` (absolute path of the PET and MR input nifti files)\n",
    "- ```coreg_inputs``` whether to apply rigid coregistration between PET and MR volumes using mutual information\n",
    "- ```crop_mr``` whether to crop both volumes to the bounding box of the MR (usefule to limit memory usage)\n",
    "- ```output_fname``` absolute path of the nifti file for the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10BENAxbbgkH"
   },
   "outputs": [],
   "source": [
    "# inputs (adapt to your needs)\n",
    "\n",
    "# the name of the trained CNN\n",
    "model_name = '200824_mae_osem_psf_bet_10'\n",
    "\n",
    "# we use a simulated demo data included in pyapetnet (based on the brainweb phantom)\n",
    "mydata_dir = '.'\n",
    "pet_fname  = os.path.join(mydata_dir, 'brainweb_06_osem.nii')\n",
    "mr_fname   = os.path.join(mydata_dir, 'brainweb_06_t1.nii')\n",
    "\n",
    "# preprocessing parameters\n",
    "\n",
    "coreg_inputs = True  # rigidly coregister PET and MR using mutual information\n",
    "crop_mr      = True   # crop the input to the support of the MR (saves memory + speeds up the computation)\n",
    "\n",
    "# the name of the ouput file\n",
    "output_fname =  f'prediction_{model_name}.nii'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5jI9bMRNNEV"
   },
   "source": [
    "## (5) Load the pre-trained CNN (model)\n",
    "Now we can load the pretrained model. pyapetnet includes a few preprained models that are installed all installed\n",
    "at <br>\n",
    "```os.path.join(os.path.dirname(pyapetnet.__file__),'trained_models')```<br>\n",
    "where ```pyapetnet.__file__``` points to the install path of pyapetnet.\n",
    "\n",
    "A more detailed description of all models can be found at <br>\n",
    "https://github.com/gschramm/pyapetnet/blob/master/pyapetnet/trained_models/model_description.md\n",
    "\n",
    "The dummy dictionary ```custom_objects``` is needed since the model definition depends on 2 custom loss functions (related to SSIM). For inference the loss fucntions are not needed with is why we pass a dummy dictionary.\n",
    "\n",
    "Last but not least, we read the internal voxel size used to train the model. This is necessary to correctly pre-process the input data (which comes usually in a different voxel size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nGzaQw1Eju_"
   },
   "outputs": [],
   "source": [
    "# load the trained CNN and its internal voxel size used for training\n",
    "model_abs_path = os.path.join(os.path.dirname(pyapetnet.__file__),'trained_models',model_name)\n",
    "\n",
    "model = tf.keras.models.load_model(model_abs_path, custom_objects = {'ssim_3d_loss': None,'mix_ssim_3d_mae_loss': None})\n",
    "                   \n",
    "# load the voxel size used for training\n",
    "with open(os.path.join(model_abs_path,'config.json')) as f:\n",
    "  cfg = json.load(f)\n",
    "  training_voxsize = cfg['internal_voxsize']*np.ones(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTi_LWFmNF8J"
   },
   "source": [
    "## (6) Load and preprocess the input PET and MR volumes\n",
    "\n",
    "Finally, ee load the data from the input nifti files. The preprocessing function rigidly coregisters the inputs,\n",
    "interpolates the volumes to the internal voxel size of the CNN, crops the volumes to the MR support, and does an intensity normalization (division by 99.9% percentile). We use the 99.99% percentile since it is more robust for noisy (PET) volumes.\n",
    "\n",
    "**The voxelsize of the input volumes is deduced from the affine transforamtion stored in the nifti header. Make sure that the affine stored there is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "SumoESLqL6iL",
    "outputId": "1c24b4ac-1879-4ea8-db38-d7f61d9e10d4"
   },
   "outputs": [],
   "source": [
    "# load and preprocess the input PET and MR volumes\n",
    "pet, pet_affine = load_nii_in_ras(pet_fname)\n",
    "mr, mr_affine   = load_nii_in_ras(mr_fname)\n",
    "\n",
    "# preprocess the input volumes (coregistration, interpolation and intensity normalization)\n",
    "pet_preproc, mr_preproc, o_aff, pet_max, mr_max = preprocess_volumes(pet, mr, \n",
    "  pet_affine, mr_affine, training_voxsize, perc = 99.99, coreg = coreg_inputs, crop_mr = crop_mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Show and check pre-processed Input data\n",
    "\n",
    "Before passing the PET and MR input volumes to the loaded CNN, it is a good idea to check whether both volumes were correctly pre-processed. If the pre-processing was successfull, the volumes should be well aligned, should be interpolated to the internal voxelsize of the CNN, and their 99.99% percentile should be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'PET 99.99% percentile {np.percentile(pet_preproc,99.99):.3f}')\n",
    "print(f'PET 99.99% percentile {np.percentile(mr_preproc,99.99):.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize = (9,6))\n",
    "ax[0,0].imshow(pet_preproc[:,::-1,pet_preproc.shape[2]//2].T, cmap = plt.cm.Greys, vmax = 1)\n",
    "ax[0,1].imshow(pet_preproc[:,pet_preproc.shape[1]//2,::-1].T, cmap = plt.cm.Greys, vmax = 1)\n",
    "ax[0,2].imshow(pet_preproc[pet_preproc.shape[0]//2,:,::-1].T, cmap = plt.cm.Greys, vmax = 1)\n",
    "ax[1,0].imshow(mr_preproc[:,::-1,pet_preproc.shape[2]//2].T, cmap = plt.cm.Greys_r, vmax = 1)\n",
    "ax[1,1].imshow(mr_preproc[:,pet_preproc.shape[1]//2,::-1].T, cmap = plt.cm.Greys_r, vmax = 1)\n",
    "ax[1,2].imshow(mr_preproc[pet_preproc.shape[2]//2,:,::-1].T, cmap = plt.cm.Greys_r, vmax = 1)\n",
    "\n",
    "for axx in ax.flatten(): axx.set_axis_off()\n",
    "\n",
    "ax[0,1].set_title('preprocessed input PET')\n",
    "ax[1,1].set_title('preprocessed input MR')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3imUqw2qPHE-"
   },
   "source": [
    "## (8) Running the actual CNN prediction\n",
    "\n",
    "Once the data is read and preprocesed we can run the actual prediction.\n",
    "The input to the pyapetnet models is a python list containing two \"tensors\" (the preprocessed PET and MR volumes). The dimensions of both tensors are (1,n0,n1,n2,1) where n0,n1,n2 are the spatial dimensions of the pre-processed volumes. The left most dimension is the batch size (1 in our case) and the right most dimension is the number of input channels / features (1 in our case). \n",
    "\n",
    "We decided to input two (1,n0,n1,n2,1) tensors instead of one (1,n0,n1,n2,2) tensor since in the first layer, since in the first layers we decided to learn separte PET and MR features. \n",
    "\n",
    "Based on the design of the model, there is no restiction on the spatial input shape (n0,n1,n2) provided that enough GPU/CPU memory is available.\n",
    "\n",
    "Using a recent Nvidia GPU, this step should take roughly 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvloGXYiPKRu"
   },
   "outputs": [],
   "source": [
    "# the actual CNN prediction\n",
    "x = [np.expand_dims(np.expand_dims(pet_preproc,0),-1), np.expand_dims(np.expand_dims(mr_preproc,0),-1)]\n",
    "pred = model.predict(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2XH9MdOatTH"
   },
   "source": [
    "## (7) Undo the intensity normalization\n",
    "\n",
    "We undo the intensity normalization that was applied during pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PFKRt_4awLu"
   },
   "outputs": [],
   "source": [
    "pred *= pet_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClQSF-bFbmyO"
   },
   "source": [
    "## (8) Save the volumes\n",
    "\n",
    "We save the pre-processed volumes and the prediction to nifti files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95s5HaqGbqd8"
   },
   "outputs": [],
   "source": [
    "nib.save(nib.Nifti1Image(pet_preproc, o_aff), 'pet_preproc.nii')\n",
    "nib.save(nib.Nifti1Image(mr_preproc, o_aff), 'mr_preproc.nii')\n",
    "nib.save(nib.Nifti1Image(pred, o_aff), f'prediction_{model_name}.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06mi6lgROvxj"
   },
   "source": [
    "## (9) Display the input and the prediction\n",
    "\n",
    "Finally we display the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "IElO4PdzMKS6",
    "outputId": "c56a5636-b00e-4de8-ea86-90dd1e3190d8"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize = (9,9))\n",
    "ax[0,0].imshow(pet_preproc[:,::-1,pet_preproc.shape[2]//2].T, cmap = plt.cm.Greys, vmax = 1)\n",
    "ax[0,1].imshow(pet_preproc[:,pet_preproc.shape[1]//2,::-1].T, cmap = plt.cm.Greys, vmax = 1)\n",
    "ax[0,2].imshow(pet_preproc[pet_preproc.shape[0]//2,:,::-1].T, cmap = plt.cm.Greys, vmax = 1)\n",
    "ax[1,0].imshow(mr_preproc[:,::-1,pet_preproc.shape[2]//2].T, cmap = plt.cm.Greys_r, vmax = 1)\n",
    "ax[1,1].imshow(mr_preproc[:,pet_preproc.shape[1]//2,::-1].T, cmap = plt.cm.Greys_r, vmax = 1)\n",
    "ax[1,2].imshow(mr_preproc[pet_preproc.shape[2]//2,:,::-1].T, cmap = plt.cm.Greys_r, vmax = 1)\n",
    "ax[2,0].imshow(pred[:,::-1,pet_preproc.shape[2]//2].T, cmap = plt.cm.Greys, vmax = pet_max)\n",
    "ax[2,1].imshow(pred[:,pet_preproc.shape[1]//2,::-1].T, cmap = plt.cm.Greys, vmax = pet_max)\n",
    "ax[2,2].imshow(pred[pet_preproc.shape[0]//2,:,::-1].T, cmap = plt.cm.Greys, vmax = pet_max)\n",
    "for axx in ax.flatten(): axx.set_axis_off()\n",
    "\n",
    "ax[0,1].set_title('pre-processed input PET')\n",
    "ax[1,1].set_title('pre-processed input MR')\n",
    "ax[2,1].set_title('predicted MAP Bowsher')\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pyapetnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
